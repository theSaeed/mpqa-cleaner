{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ZSbuSyCooU5a"},"outputs":[],"source":["# Doclists\n","\n","ULA_SUBSET_DOCS = [\n","  'ula/119CWL041', 'ula/RindnerBonnie', 'ula/HistoryGreek',\n","  'ula/Article247_3500', 'ula/NapierDianne', 'ula/sw2071-UTF16-ms98-a-trans',\n","  'ula/118CWL050', 'ula/114CUL059', 'ula/110CYL067', 'ula/PolkMaria',\n","  'ula/116CUL034', 'ula/115CVL037', 'ula/118CWL049', 'ula/Article247_66',\n","  'ula/110CYL068', 'ula/113CWL017', 'ula/112C-L015', 'ula/115CVL036',\n","  'ula/115CVL035', 'ula/Article247_328', 'ula/114CUL060', 'ula/112C-L012',\n","  'ula/118CWL048', 'ula/ReidSandra', 'ula/112C-L016', 'ula/HistoryJerusalem',\n","  'ula/110CYL070', 'ula/sw2014-UTF16-ms98-a-trans', 'ula/112C-L014',\n","  'ula/117CWL008', 'ula/sw2078-UTF16-ms98-a-trans', 'ula/110CYL071',\n","  'ula/114CUL057', 'ula/116CUL032', 'ula/110CYL069', 'ula/117CWL009',\n","  'ula/110CYL072', 'ula/chapter-10', 'ula/116CUL033', 'ula/ch5',\n","  'ula/sw2015-ms98-a-trans', 'ula/113CWL018', 'ula/110CYL200',\n","  'ula/Article247_327', 'ula/114CUL058', 'ula/112C-L013', 'ula/Article247_500',\n","  'ula/Article247_400'\n","]\n","\n","ULA_LU_SUBSET_DOCS = [\n","  'ula/A1.E2-NEW', 'ula/wsj_1640.mrg-NEW', 'ula/AFGP-2002-600045-Trans',\n","  'ula/20000410_nyt-NEW', 'ula/20000415_apw_eng-NEW',\n","  'ula/AFGP-2002-602187-Trans', 'ula/20000815_AFP_ARB.0084.IBM-HA-NEW',\n","  'ula/CNN_AARONBROWN_ENG_20051101_215800.partial-NEW', 'ula/20000424_nyt-NEW',\n","  'ula/20000419_apw_eng-NEW', 'ula/20000416_xin_eng-NEW',\n","  'ula/enron-thread-159550', 'ula/wsj_2465', 'ula/AFGP-2002-600002-Trans',\n","  'ula/ENRON-pearson-email-25jul02', 'ula/im_401b_e73i32c22_031705-2',\n","  'ula/A1.E1-NEW', 'ula/CNN_ENG_20030614_173123.4-NEW-1',\n","  'ula/20000420_xin_eng-NEW', 'ula/IZ-060316-01-Trans-1',\n","  'ula/sw2025-ms98-a-trans.ascii-1-NEW', 'ula/SNO-525',\n","  'ula/AFGP-2002-600175-Trans', 'ula/602CZL285-1'\n","]\n","\n","XBANK_DOCS = [\n","  'xbank/wsj_0904', 'xbank/wsj_0760', 'xbank/wsj_0713', 'xbank/wsj_0709',\n","  'xbank/wsj_0706', 'xbank/wsj_0662', 'xbank/wsj_0558', 'xbank/wsj_0555',\n","  'xbank/wsj_0551', 'xbank/wsj_0542', 'xbank/wsj_0541', 'xbank/wsj_0332',\n","  'xbank/wsj_0292', 'xbank/wsj_0189', 'xbank/wsj_0316', 'xbank/wsj_0175',\n","  'xbank/wsj_0321', 'xbank/wsj_0176', 'xbank/wsj_0173', 'xbank/wsj_0026',\n","  'xbank/wsj_0324', 'xbank/wsj_0187', 'xbank/wsj_0356', 'xbank/wsj_0325',\n","  'xbank/wsj_0340', 'xbank/wsj_0679', 'xbank/wsj_0695', 'xbank/wsj_0661',\n","  'xbank/wsj_0570', 'xbank/wsj_0557', 'xbank/wsj_0751', 'xbank/wsj_0805',\n","  'xbank/wsj_0762', 'xbank/wsj_0736', 'xbank/wsj_0806', 'xbank/wsj_1040',\n","  'xbank/wsj_1039', 'xbank/wsj_1042', 'xbank/wsj_0568', 'xbank/wsj_0778',\n","  'xbank/wsj_0160', 'xbank/wsj_0136', 'xbank/wsj_0135', 'xbank/wsj_0127',\n","  'xbank/wsj_0122', 'xbank/wsj_0032', 'xbank/wsj_0150', 'xbank/wsj_0165',\n","  'xbank/wsj_0157', 'xbank/wsj_0151', 'xbank/wsj_0685', 'xbank/wsj_0168',\n","  'xbank/wsj_0167', 'xbank/wsj_0161', 'xbank/wsj_0152', 'xbank/wsj_0073',\n","  'xbank/wsj_0068', 'xbank/wsj_0171', 'xbank/wsj_0144', 'xbank/wsj_0991',\n","  'xbank/wsj_0923', 'xbank/wsj_0907', 'xbank/wsj_0811', 'xbank/wsj_0667',\n","  'xbank/wsj_0534', 'xbank/wsj_0924', 'xbank/wsj_0815', 'xbank/wsj_1038',\n","  'xbank/wsj_1035', 'xbank/wsj_1033', 'xbank/wsj_0527', 'xbank/wsj_0928',\n","  'xbank/wsj_0973', 'xbank/wsj_0950', 'xbank/wsj_0927', 'xbank/wsj_0376',\n","  'xbank/wsj_0660', 'xbank/wsj_0650', 'xbank/wsj_0266', 'xbank/wsj_0006',\n","  'xbank/wsj_0768', 'xbank/wsj_1073', 'xbank/wsj_0816', 'xbank/wsj_0610',\n","  'xbank/wsj_0583'\n","]\n","\n","################################################################################\n","\n","# Config\n","\n","RUNTIME_TYPE = 'COLAB'\n","EXPERIMENT_NAME = 'intensity'\n","IGNORED_DOCS = ULA_SUBSET_DOCS + ULA_LU_SUBSET_DOCS + XBANK_DOCS # e.g. set it to ['non_fbis/06.11.16-17420'] to remove all objects with that specific document id\n","CLASSES = ['agreement', 'arguing', 'expressive_subjectivity', 'intention', 'sentiment']\n","ANNOT_DICT = {'expressive_subjectivity': 'expressive subjectivity',\n","              'speculation': 'speculation', 'other_attitude': 'other',\n","              'intention': 'intention', 'arguing': 'arguing',\n","              'agreement': 'agreement', 'sentiment': 'sentiment'}\n","\n","CLASSES = {'medium': [0, 1, 0], 'medium-high': [0, 1, 1], 'low': [1, 0, 0],\n","           'high': [0, 0, 1], 'low-medium': [1, 1, 0],\n","           'high-extreme': [0, 0, 1], 'extreme': [0, 0, 1]}\n","\n","MODEL_NAME = 'bert-base-uncased'\n","FREEZE_LAYER_COUNT = None\n","LEARNING_RATE = 1e-5\n","FP16 = False\n","LOCAL_RANK = -1\n","FP16_OPT_LEVEL = 'O1'\n","BATCH_SIZE = 16\n","TRAIN_BATCH_SIZE = 16\n","VAL_BATCH_SIZE = 16\n","TEST_BATCH_SIZE = 1\n","DROPOUT = 0.1\n","NUM_TRAIN_EPOCHS = 24\n","SEED = 0\n","DATA = 'MPQA2.0_cleaned'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fmmVTB1VL2rP"},"outputs":[],"source":["!pip install transformers[sentencepiece]\n","!pip install torchinfo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MmK-bYvtsUDc"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UhdfMc0XsUjs"},"outputs":[],"source":["if RUNTIME_TYPE == 'COLAB':\n","    %pip install transformers[sentencepiece]\n","    %pip install torchinfo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wbY_2kwOsY9U"},"outputs":[],"source":["# To assure deterministic results\n","import os\n","os.environ['PYTHONHASHSEED'] = str(SEED)\n","os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":4096:8\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RT4rYJSjsdaE"},"outputs":[],"source":["import gc\n","import sys\n","import random\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import pandas as pd\n","from transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer, DataCollatorWithPadding, EarlyStoppingCallback, AutoConfig\n","from sklearn.model_selection import train_test_split, StratifiedKFold\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","import json\n","from urllib.request import urlopen\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import statistics\n","import os\n","from tqdm import tqdm\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","from datetime import datetime\n","\n","%matplotlib inline\n","%config InlineBackend.figure_format='retina'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H9jeB-jRMgqE"},"outputs":[],"source":["import random\n","import time\n","from nltk import word_tokenize\n","import nltk\n","from nltk.corpus import stopwords\n","import re\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d59EL--jss8y"},"outputs":[],"source":["# Start timer\n","\n","start_time = datetime.now()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P9TIakYIsuKi"},"outputs":[],"source":["# Support for third-party widgets\n","\n","if RUNTIME_TYPE == 'COLAB':\n","    from google.colab import output\n","    output.enable_custom_widget_manager()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d7uBaI38s0iO"},"outputs":[],"source":["# Setup device\n","\n","device_string = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device_hf = 0 if torch.cuda.is_available() else -1\n","device = torch.device(device_string)\n","print(\"Device:\", device)\n","NUM_WORKERS = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1UEbjlIXgl1X"},"outputs":[],"source":["data_name_to_google_drive_url = {\n","    \"MPQA2.0_cleaned_testset_fold_1\": \"[Replace proper link here.]\",\n","    \"MPQA2.0_cleaned_testset_fold_2\": \"[Replace proper link here.]\",\n","    \"MPQA2.0_cleaned_testset_fold_3\": \"[Replace proper link here.]\",\n","    \"MPQA2.0_cleaned_testset_fold_4\": \"[Replace proper link here.]\",\n","    \"MPQA2.0_cleaned_testset_fold_5\": \"[Replace proper link here.]\",\n","    \"MPQA2.0_cleaned_trainset_fold_1\": \"[Replace proper link here.]\",\n","    \"MPQA2.0_cleaned_trainset_fold_2\": \"[Replace proper link here.]\",\n","    \"MPQA2.0_cleaned_trainset_fold_3\": \"[Replace proper link here.]\",\n","    \"MPQA2.0_cleaned_trainset_fold_4\": \"[Replace proper link here.]\",\n","    \"MPQA2.0_cleaned_trainset_fold_5\": \"[Replace proper link here.]\",\n","    \"MPQA2.0_cleaned_validationset_fold_1\": \"[Replace proper link here.]\",\n","    \"MPQA2.0_cleaned_validationset_fold_2\": \"[Replace proper link here.]\",\n","    \"MPQA2.0_cleaned_validationset_fold_3\": \"[Replace proper link here.]\",\n","    \"MPQA2.0_cleaned_validationset_fold_4\": \"[Replace proper link here.]\",\n","    \"MPQA2.0_cleaned_validationset_fold_5\": \"[Replace proper link here.]\"\n","\n","}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rftfNM0kLEXY"},"outputs":[],"source":["# Get direct download link\n","def get_download_url_from_google_drive_url(google_drive_url):\n","    return f'https://drive.google.com/uc?id={google_drive_url.split(\"/\")[5]}&export=download&confirm=t'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2u166MmiLI6n"},"outputs":[],"source":["def set_seed():\n","    np.random.seed(SEED)\n","    torch.manual_seed(SEED)\n","    torch.cuda.manual_seed(SEED)\n","    torch.backends.cudnn.deterministic = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"msEpoHpzYqAK"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","###########\n","init_token = ' ' + tokenizer.cls_token\n","eos_token = ' ' + tokenizer.sep_token"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MHKrgyGDqfYO"},"outputs":[],"source":["if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n","    print('Device name:', torch.cuda.get_device_name(0))\n","\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"evbOXodbvovY"},"outputs":[],"source":["def make_token_type_id1(token_types, ids, att, txt=None):\n","  tmp1 = torch.full(ids.shape, 0)\n","  tmp2 = torch.full(ids.shape, 1)\n","  tmp3 = torch.full(ids.shape, 0)\n","  tmp4 = torch.full((ids.shape[0], 2), 0)\n","  tmp5 = torch.full(ids.shape, 0)\n","\n","  for j in range(ids.shape[0]):\n","    beg = 0\n","    end = ids[j].shape[0]\n","\n","\n","    for i in range(beg, ids[j].shape[0]):\n","      if ids[j][i] == 102:\n","        beg = i+1\n","        break\n","    for i in range(beg, ids[j].shape[0]):\n","      if ids[j][i] == 102:\n","        beg = i+1\n","        break\n","\n","    for i in range(beg, ids[j].shape[0]):\n","      if ids[j][i] == 102:\n","        end = i\n","        break\n","\n","    for i in range(beg, end):\n","      token_types[j][i] = 1\n","\n","\n","\n","    try:\n","\n","        tmp1[j][0:beg-1] = token_types[j][0:beg-1]\n","        tmp1[j][beg-1:end-1] = token_types[j][beg:end]\n","        if end+1 < token_types[j].shape[0]:\n","          tmp1[j][end-1:-2] = token_types[j][end+1:]\n","\n","          tmp1[j][-2] = 0\n","          tmp1[j][-1] = 0\n","\n","\n","        tmp2[j][0:beg-1] = att[j][0:beg-1]\n","        tmp2[j][beg-1:end-1] = att[j][beg:end]\n","        if end+1 <token_types[j].shape[0]:\n","          tmp2[j][end-1:-2] = att[j][end+1:]\n","          tmp2[j][-2] = 0\n","          tmp2[j][-1] = 0\n","\n","        tmp3[j][0:beg-1] = ids[j][0:beg-1]\n","        tmp3[j][beg-1:end-1] = ids[j][beg:end]\n","        if end+1 <token_types[j].shape[0]:\n","          tmp3[j][end-1:-2] = ids[j][end+1:]\n","          tmp3[j][-2] = 0\n","          tmp3[j][-1] = 0\n","\n","\n","\n","        tmp4[j][0] = beg - 1\n","        tmp4[j][1] = end - 1\n","    except:\n","        pass\n","\n","  return tmp1, tmp2, tmp3, tmp4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4AwHEihUpajb"},"outputs":[],"source":["def make_token_type_id2(token_types, ids, att, txt=None):\n","  tmp1 = torch.full(ids.shape, 0)\n","  tmp2 = torch.full(ids.shape, 1)\n","  tmp3 = torch.full(ids.shape, 0)\n","  tmp4 = torch.full((ids.shape[0], 2), 0)\n","  tmp5 = torch.full(ids.shape, 0)\n","\n","\n","  for j in range(ids.shape[0]):\n","    beg = 0\n","    end = ids[j].shape[0]\n","\n","    for i in range(ids[j].shape[0]):\n","      if ids[j][i] == 102:\n","        beg = i+1\n","        break\n","    for i in range(beg, ids[j].shape[0]):\n","      if ids[j][i] == 102:\n","        beg = i+1\n","        break\n","\n","\n","    for i in range(beg, end):\n","      token_types[j][i] = 1\n","\n","\n","\n","    try:\n","\n","        tmp1[j][0:beg] = token_types[j][0:beg]\n","        tmp1[j][beg:] = token_types[j][beg:]\n","\n","\n","\n","        tmp2[j][0:beg] = att[j][0:beg]\n","        tmp2[j][beg:] = att[j][beg:]\n","\n","\n","        tmp3[j][0:beg] = ids[j][0:beg]\n","        tmp3[j][beg:] = ids[j][beg:]\n","\n","    except:\n","        pass\n","\n","  return tmp1, tmp2, tmp3, tmp4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y42T3TDN0pg6"},"outputs":[],"source":["def make_token_type_id3(token_types, ids, att, txt=None):\n","\n","  tmp4 = torch.full((ids.shape[0], 2), 0)\n","\n","  return token_types, att, ids, tmp4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0IcLVd0zudSn"},"outputs":[],"source":["class SentimentDataset(Dataset):\n","\n","  def __init__(self, texts, targets, annot=None, max_len=200, targets_reg=None):\n","    self.texts = texts\n","    self.targets = targets\n","    self.tokenizer = tokenizer\n","    self.max_len = max_len\n","    self.annot = annot\n","\n","  def __len__(self):\n","    return len(self.texts)\n","\n","  def __getitem__(self, item):\n","    text = str(self.texts[item])\n","    target = self.targets[item]\n","    annot = self.annot[item]\n","\n","\n","    encoding = self.tokenizer.encode_plus(\n","      text,\n","      max_length=self.max_len,\n","      add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n","      return_token_type_ids=True,\n","      padding = 'max_length',  # Pad to longest in batch.\n","      truncation=True,\n","      return_attention_mask=True,\n","      return_tensors='pt'  # Return PyTorch tensors\n","    )\n","\n","\n","    token_type_ids, attention_mask,  ids, beg_end = make_token_type_id2(encoding['token_type_ids'], encoding['input_ids'], encoding['attention_mask'], text)\n","\n","\n","    return {\n","      'text': text,\n","      'type_ids': token_type_ids.flatten(),\n","      'input_ids': ids.flatten(),\n","      'attention_mask': attention_mask.flatten(),\n","      'beg_end': beg_end.flatten(),\n","      'targets': torch.tensor(target, dtype=torch.float),\n","      'annot': torch.tensor(annot)\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MCKtUflBg7tr"},"outputs":[],"source":["def create_data_loader(texts, targets, max_len, batch_size, annot=None):\n","  ds = SentimentDataset(\n","    texts=texts,\n","    targets=targets,\n","    max_len=max_len,\n","    annot=annot\n","  )\n","\n","  return DataLoader(\n","    ds,\n","    batch_size=batch_size,\n","    num_workers=0\n","  )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uoOl25HtmpLC"},"outputs":[],"source":["def initilize_model(model_name='bert-base-uncased', freeze_embedding=False):\n","\n","    # Bind model to `device` (GPU/CPU)\n","    model_hf = AutoModel.from_pretrained(\n","        MODEL_NAME, resume_download=True,\n","        config=AutoConfig.from_pretrained(MODEL_NAME)\n","    )\n","\n","    return model_hf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_gYHRE5Snzz_"},"outputs":[],"source":["import torch.optim as optim\n","\n","def load_optimizer(model, learning_rate=1e-5, weight_decay=0):\n","\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","\n","    return optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qzwblOvGlDt_"},"outputs":[],"source":["class SentimentClassifier(nn.Module):\n","\n","  def __init__(self, bert_model):\n","    super(SentimentClassifier, self).__init__()\n","    self.bert = bert_model\n","    hidden_dim = self.bert.config.hidden_size\n","    dropout_rate = 0.1\n","    n_classes = 3\n","    self.drop = nn.Dropout(p=dropout_rate)\n","\n","    self.hidden_size = 128\n","\n","    self.rnn2 = nn.GRU(hidden_dim,\n","                      self.hidden_size,\n","                      num_layers = 1,\n","                      bidirectional = True,\n","                      batch_first = True,\n","                      dropout = 0)\n","\n","\n","    self.out_rnn2 = nn.Linear(self.hidden_size * 2, 3)\n","\n","  def forward(self, input_ids, token_type_ids, attention_mask, beg_end=None, type_vector=None):\n","\n","\n","    #pooled_output = model_outs[1]\n","    #last_hidden_state = model_outs[0]\n","\n","    bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n","    bert_output = bert_output[0] # last hidden state with the shape of (batch_size, sequence_length, hidden_size)\n","\n","    #\n","    rnn_results = torch.full((input_ids.shape[0], 2*self.hidden_size), 0, dtype=torch.float).to(device)\n","    #\n","    try:\n","      for b in range(input_ids.shape[0]):\n","          #GRU\n","          #For EXP\n","          output, h_n = self.rnn2(bert_output[b:b+1,beg_end[b][0]:beg_end[b][1], :])\n","\n","          rnn_results[b, :] = self.drop(torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim = 1))\n","    except:\n","        print(' ***************** ')\n","        print(bert_output[b].shape)\n","        print(input_ids[b].shape)\n","        print(input_ids[b])\n","        print(token_type_ids[b])\n","        print(beg_end[b])\n","        print(tokenizer.convert_ids_to_tokens(input_ids[b]))\n","        print(' ***************** ')\n","    #\n","    prediction = self.out_rnn2(rnn_results)\n","\n","    return prediction, output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dWflJzQM6HNI"},"outputs":[],"source":["def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fke-WeM25oHA"},"outputs":[],"source":["def f1_calculator(y, outputs):\n","\n","    whole = 1.0\n","    with_penalty = 0\n","    half_false = 1\n","    thresh = 0.5\n","    trues = {'medium': 0, 'medium-high': 0, 'low': 0, 'high': 0, 'low-medium': 0}\n","    cnt = {'medium': 0, 'medium-high': 0, 'low': 0, 'high': 0, 'low-medium': 0}\n","    falses = {'medium': 0, 'medium-high': 0, 'low': 0, 'high': 0, 'low-medium': 0}\n","\n","\n","    for i in range(len(outputs)):\n","\n","        t = outputs[i].clone().detach()\n","\n","        max_index = 0\n","        max_value = outputs[i][max_index]\n","        for p in range(3):\n","          max_index = p if outputs[i][p] > outputs[i][max_index] else max_index\n","\n","        for p in range(3):\n","          t[p] = 1 if p == max_index else 0\n","\n","\n","\n","        ##\n","        if t[0] == 1 and  t[1] == 0 and  t[2] == 0:\n","          if y[i][0] == 0 and  y[i][1] == 0 and  y[i][2] == 1:\n","              falses['low'] += 1\n","          if y[i][0] == 0 and  y[i][1] == 1 and  y[i][2] == 0:\n","              falses['low'] += half_false\n","          if y[i][0] == 0 and  y[i][1] == 1 and  y[i][2] == 1:\n","              falses['low'] += half_false\n","\n","        #\n","        if t[0] == 1 and  t[1] == 1 and  t[2] == 0:\n","          if y[i][0] == 0 and  y[i][1] == 0 and  y[i][2] == 1:\n","              falses['low-medium'] += 1\n","          if y[i][0] == 0 and  y[i][1] == 1 and  y[i][2] == 0:\n","              falses['low-medium'] += 0\n","          if y[i][0] == 1 and  y[i][1] == 0 and  y[i][2] == 0:\n","              falses['low-medium'] += 0\n","\n","        #\n","        if t[0] == 0 and  t[1] == 1 and  t[2] == 0:\n","          if y[i][0] == 0 and  y[i][1] == 0 and  y[i][2] == 1:\n","              falses['medium'] += half_false\n","          if y[i][0] == 1 and  y[i][1] == 0 and  y[i][2] == 0:\n","              falses['medium'] += half_false\n","\n","\n","        #\n","        if t[0] == 0 and  t[1] == 1 and  t[2] == 1:\n","          if y[i][0] == 1 and  y[i][1] == 0 and  y[i][2] == 0:\n","              falses['medium-high'] += 1\n","          if y[i][0] == 1 and  y[i][1] == 1 and  y[i][2] == 0:\n","              falses['medium-high'] += 0\n","          if y[i][0] == 1 and  y[i][1] == 0 and  y[i][2] == 0:\n","              falses['medium-high'] += 0\n","\n","        #\n","        if t[0] == 0 and  t[1] == 0 and  t[2] == 1:\n","          if y[i][0] == 1 and  y[i][1] == 0 and  y[i][2] == 0:\n","              falses['high'] += 1\n","          if y[i][0] == 1 and  y[i][1] == 1 and  y[i][2] == 0:\n","              falses['high'] += 1\n","          if y[i][0] == 0 and  y[i][1] == 1 and  y[i][2] == 1:\n","              falses['high'] += half_false\n","          if y[i][0] == 0 and  y[i][1] == 1 and  y[i][2] == 0:\n","              falses['high'] += half_false\n","\n","\n","        ##\n","\n","\n","        if y[i][0] == 1 and  y[i][1] == 0 and  y[i][2] == 0:\n","          cnt['low'] += 1\n","\n","          if t[0] == 1 and  t[1] == 0 and  t[2] == 0:\n","              trues['low'] += whole\n","          elif t[0] == 1 and  t[1] == 1 and  t[2] == 0:\n","              trues['low'] += with_penalty\n","\n","\n","        elif y[i][0] == 0 and  y[i][1] == 1 and  y[i][2] == 0:\n","          cnt['medium'] += 1\n","\n","          if t[0] == 0 and  t[1] == 1 and  t[2] == 0:\n","              trues['medium'] += whole\n","          else:\n","              trues['medium'] += with_penalty\n","\n","\n","\n","        elif y[i][0] == 0 and  y[i][1] == 0 and  y[i][2] == 1:\n","          cnt['high'] += 1\n","\n","          if t[0] == 0 and  t[1] == 0 and  t[2] == 1:\n","              trues['high'] += whole\n","          elif t[0] == 0 and  t[1] == 1 and  t[2] == 1:\n","              trues['high'] += with_penalty\n","\n","        elif y[i][0] == 1 and  y[i][1] == 1 and  y[i][2] == 0:\n","          cnt['low-medium'] += 1\n","\n","          if (t[0] == 1 or  t[1] == 1) and  t[2] == 0:\n","              trues['low-medium'] += whole\n","\n","\n","\n","        elif y[i][0] == 0 and  y[i][1] == 1 and  y[i][2] == 1:\n","          cnt['medium-high'] += 1\n","\n","          if (t[1] == 1 or  t[2] == 1) and  t[0] == 0:\n","              trues['medium-high'] += whole\n","\n","\n","    del t\n","\n","    weighted_f1 = 0\n","    weights = 0\n","    for intensity_class in trues.keys():\n","        try:\n","            intensity_class_precision = trues[intensity_class] / (trues[intensity_class] + falses[intensity_class])\n","        except:\n","            intensity_class_precision = 1\n","        try:\n","            intensity_class_recall = trues[intensity_class] / cnt[intensity_class]\n","        except:\n","            intensity_class_recall = 1\n","        intensity_class_f1 = statistics.harmonic_mean([intensity_class_precision, intensity_class_recall])\n","        weighted_f1 += intensity_class_f1 * cnt[intensity_class]\n","        weights += cnt[intensity_class]\n","    intensity_weighted_f1 = weighted_f1 / weights\n","\n","    return intensity_weighted_f1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LYbvAd7u4Nwr"},"outputs":[],"source":["def train_epoch(\n","  model,\n","  data_loaders,\n","  loss_fn,\n","  optimizer,\n","  device,\n","  n_examples,\n","  scheduler=None,\n","  epoch_num=1\n","):\n","  model = model.train()\n","  preds_all = []\n","  targets_all = []\n","\n","  losses = []\n","  correct_predictions = 0\n","  f1 = 0\n","  for data_loader in data_loaders:\n","    for d in data_loader:\n","      input_ids = d[\"input_ids\"].to(device)\n","      attention_mask = d[\"attention_mask\"].to(device)\n","      type_ids = d[\"type_ids\"].to(device)\n","      targets = d[\"targets\"].to(device)\n","      beg_end = d[\"beg_end\"].to(device)\n","      annot = d[\"annot\"].to(device)\n","\n","      #\n","\n","\n","      outputs, output = model(\n","        input_ids=input_ids,\n","        token_type_ids=type_ids,\n","        attention_mask=attention_mask,\n","        beg_end=beg_end,\n","        type_vector=annot\n","      )\n","\n","\n","      extracted_intensity_preds_argmax = torch.argmax(targets, dim=1).flatten()\n","\n","      x = output\n","      x = x.float()\n","      loss = loss_fn(x, extracted_intensity_preds_argmax)\n","\n","      # Get the predictions\n","      extracted_intensity_preds_argmax = torch.argmax(outputs, dim=1).flatten()\n","\n","      # Calculate the accuracy rate\n","      intensity_score = 0\n","      for i in range(len(extracted_intensity_preds_argmax)):\n","        if (targets[i, extracted_intensity_preds_argmax[i]] == 1):\n","          intensity_score += 1\n","      accuracy = ( intensity_score / len(extracted_intensity_preds_argmax) ) * 100\n","\n","      correct_predictions += intensity_score\n","\n","\n","      losses.append(loss.item())\n","\n","\n","      preds_all.extend(outputs)\n","      targets_all.extend(targets)\n","\n","      loss.backward()\n","\n","      optimizer.step()\n","\n","      optimizer.zero_grad()\n","\n","  f1 = f1_calculator(targets_all, preds_all)\n","  return correct_predictions / n_examples, np.mean(losses), f1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0EsgJ-zJ4OQB"},"outputs":[],"source":["def eval_model(model, data_loaders, loss_fn, device, n_examples):\n","  model = model.eval()\n","\n","  losses = []\n","  preds_all = []\n","  targets_all = []\n","\n","  correct_predictions = 0\n","  f1 = 0\n","\n","  for data_loader in data_loaders:\n","    with torch.no_grad():\n","      for d in data_loader:\n","        input_ids = d[\"input_ids\"].to(device)\n","        attention_mask = d[\"attention_mask\"].to(device)\n","        type_ids = d[\"type_ids\"].to(device)\n","        targets = d[\"targets\"].to(device)\n","        beg_end = d[\"beg_end\"].to(device)\n","        annot = d[\"annot\"].to(device)\n","\n","        outputs, output = model(\n","          input_ids=input_ids,\n","          token_type_ids=type_ids,\n","          attention_mask=attention_mask,\n","          beg_end=beg_end,\n","          type_vector=annot\n","        )\n","\n","\n","        # Get the predictions\n","        extracted_intensity_preds_argmax = torch.argmax(outputs, dim=1).flatten()\n","\n","\n","\n","        # Calculate the accuracy rate\n","        intensity_score = 0\n","        for i in range(len(extracted_intensity_preds_argmax)):\n","          if (targets[i, extracted_intensity_preds_argmax[i]] == 1):\n","            intensity_score += 1\n","\n","        correct_predictions += intensity_score\n","        extracted_intensity_preds_argmax = torch.argmax(targets, dim=1).flatten()\n","        x = output\n","        x = x.float()\n","        loss = loss_fn(x, extracted_intensity_preds_argmax)\n","\n","        preds_all.extend(outputs)\n","        targets_all.extend(targets)\n","\n","        losses.append(loss.item())\n","\n","  f1 = f1_calculator(targets_all, preds_all)\n","\n","  return correct_predictions/ n_examples, np.mean(losses), f1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8RnzVqCd4Qt3"},"outputs":[],"source":["import time\n","\n","# Specify loss function\n","loss_fn = nn.BCEWithLogitsLoss()\n","loss_fn = loss_fn.to(device)\n","\n","\n","def train_eval(train_data_loaders, val_data_loaders, test_data_loaders,\n","               train_set_size, val_set_size, test_set_size, epochs=4):\n","\n","  best_accuracy = 0\n","  best_f1 = 0\n","  avg_val_acc = 0\n","  avg_val_f1 = 0\n","  avg_train_acc = 0\n","  avg_train_f1 = 0\n","  ov_test_acc = 0\n","  ov_test_f1 = 0\n","\n","\n","\n","  # Start training loop\n","  print(\"Start training...\\n\")\n","\n","  for epoch in range(epochs):\n","\n","    print(f'Epoch {epoch + 1}/{epochs}')\n","    print('-' * 10)\n","    start_time = time.time()\n","\n","    train_acc, train_loss, train_f1 = train_epoch(\n","      model,\n","      train_data_loaders,\n","      loss_fn,\n","      optimizer,\n","      device,\n","      train_set_size,\n","      epoch_num=epoch\n","    )\n","\n","    avg_train_acc += train_acc\n","    avg_train_f1 += train_f1\n","\n","    print(f'Train loss {train_loss}, accuracy {train_acc}, f1 {train_f1}')\n","\n","    val_acc, val_loss, val_f1 = eval_model(\n","      model,\n","      val_data_loaders,\n","      loss_fn,\n","      device,\n","      val_set_size\n","    )\n","\n","    avg_val_acc += val_acc\n","    avg_val_f1 += val_f1\n","\n","    print(f'Val   loss {val_loss}, accuracy {val_acc}, f1 {val_f1}')\n","\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    print(f'Epoch Time: {epoch_mins}m {epoch_secs}s\\n')\n","\n","\n","    if val_acc > best_accuracy:\n","      best_accuracy = val_acc\n","      best_f1 = val_f1\n","      test_acc, test_loss, test_f1 = eval_model(\n","      model,\n","      test_data_loaders,\n","      loss_fn,\n","      device,\n","      test_set_size\n","      )\n","\n","\n","      ov_test_acc = test_acc\n","      ov_test_f1 = test_f1\n","\n","\n","\n","  return avg_train_acc / epochs, avg_train_f1 / epochs, avg_val_acc / epochs, avg_val_f1 / epochs,  ov_test_acc, ov_test_f1, best_accuracy, best_f1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K1WM4RxYwcpr"},"outputs":[],"source":["def make_dataloaders(train, val, test, y_train, y_val, y_test, train_type, val_type, test_type):\n","  max_len_cnt_train = [256]\n","  token_len_cnt_train = [0] * len(max_len_cnt_train)\n","  trains = []\n","  y_trains = []\n","  for i in range(len(max_len_cnt_train)):\n","    trains.append(list())\n","    y_trains.append(list())\n","\n","\n","  token_lens = []\n","\n","  for k in range(len(train)):\n","    tokens = tokenizer.encode(train[k], max_length=512)\n","    token_lens.append(len(tokens))\n","\n","    for i in range(len(max_len_cnt_train)):\n","      if len(tokens) <= max_len_cnt_train[i]:\n","        token_len_cnt_train[i] += 1\n","        trains[i].append(train[k])\n","        y_trains[i].append(y_train[k])\n","        break\n","\n","\n","  max_len_cnt_val = [256]\n","  token_len_cnt_val = [0] * len(max_len_cnt_val)\n","  vals = []\n","  y_vals = []\n","  for i in range(len(max_len_cnt_val)):\n","    vals.append(list())\n","    y_vals.append(list())\n","\n","  for k in range(len(val)):\n","    tokens = tokenizer.encode(val[k], max_length=512)\n","\n","    for i in range(len(max_len_cnt_val)):\n","      if len(tokens) <= max_len_cnt_val[i]:\n","        token_len_cnt_val[i] += 1\n","        vals[i].append(val[k])\n","        y_vals[i].append(y_val[k])\n","        break\n","\n","  max_len_cnt_test = max_len_cnt_val\n","  token_len_cnt_test = [0] * len(max_len_cnt_test)\n","  tests = []\n","  y_tests = []\n","  for i in range(len(max_len_cnt_test)):\n","    tests.append(list())\n","    y_tests.append(list())\n","\n","  for k in range(len(test)):\n","    tokens = tokenizer.encode(test[k], max_length=512)\n","\n","    for i in range(len(max_len_cnt_test)):\n","      if len(tokens) <= max_len_cnt_test[i]:\n","        token_len_cnt_test[i] += 1\n","        tests[i].append(test[k])\n","        y_tests[i].append(y_test[k])\n","        break\n","\n","\n","  train_data_loaders = []\n","  val_data_loaders = []\n","  test_data_loaders = []\n","\n","\n","  for i in range(len(trains)):\n","    train_data_loader = create_data_loader(trains[i], y_trains[i], max_len_cnt_train[i], BATCH_SIZE, annot=train_type)\n","    train_data_loaders.append(train_data_loader)\n","\n","\n","  for i in range(len(vals)):\n","    val_data_loader = create_data_loader(vals[i], y_vals[i], max_len_cnt_val[i], BATCH_SIZE, annot=val_type)\n","    val_data_loaders.append(val_data_loader)\n","\n","\n","  for i in range(len(tests)):\n","    test_data_loader = create_data_loader(tests[i], y_tests[i], max_len_cnt_test[i], BATCH_SIZE, annot=test_type)\n","    test_data_loaders.append(test_data_loader)\n","\n","\n","  return train_data_loaders, val_data_loaders, test_data_loaders"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rSSZybLXgVeF"},"outputs":[],"source":["path = \"/content/drive/My Drive/Overall_Final_Files/kfold/\"\n","folds_val_log  = []\n","folds_val_log_f1  = []\n","\n","folds_encrypted_test_log = []\n","folds_encrypted_test_log_f1 = []\n","\n","k_fold = 5\n","\n","for k in range(1, k_fold+1):\n","    set_seed()\n","\n","    google_drive_url = data_name_to_google_drive_url[DATA + '_trainset_fold_{}'.format(k)]\n","    data_url = get_download_url_from_google_drive_url(google_drive_url)\n","    response = urlopen(data_url)\n","    train_data = json.loads(response.read())\n","\n","    google_drive_url = data_name_to_google_drive_url[DATA + '_validationset_fold_{}'.format(k)]\n","    data_url = get_download_url_from_google_drive_url(google_drive_url)\n","    response = urlopen(data_url)\n","    val_data = json.loads(response.read())\n","\n","    google_drive_url = data_name_to_google_drive_url[DATA + '_testset_fold_{}'.format(k)]\n","    data_url = get_download_url_from_google_drive_url(google_drive_url)\n","    response = urlopen(data_url)\n","    test_data = json.loads(response.read())\n","\n","\n","    X_train_text = train_data['all']\n","    X_test_text  = test_data['all']\n","    X_val_text   = val_data['all']\n","    y_train = train_data['target']\n","    y_test  = test_data['target']\n","    y_val   = val_data['target']\n","    X_train_type = train_data['annotationType']\n","    X_test_type = test_data['annotationType']\n","    X_val_type = val_data['annotationType']\n","\n","    train_data_loaders, val_data_loaders, test_data_loaders = make_dataloaders(\n","                                                              X_train_text,\n","                                                              X_val_text,\n","                                                              X_test_text,\n","                                                              y_train,\n","                                                              y_val,\n","                                                              y_test,\n","                                                              X_train_type,\n","                                                              X_val_type,\n","                                                              X_test_type)\n","\n","\n","\n","    print(\"Training set size:\",   len(y_train))\n","    print(\"Validation set size:\", len(y_val))\n","    print(\"Test set size:\",       len(y_test))\n","\n","    del X_train_text, X_val_text, X_test_text\n","\n","\n","    print(f'\\n\\033[1mFold {k}:\\033[0m')\n","\n","    model_hf = initilize_model(model_name=MODEL_NAME, freeze_embedding=False)\n","\n","    model = SentimentClassifier(model_hf)\n","    model = model.to(device)\n","\n","    optimizer = load_optimizer(model, learning_rate=LEARNING_RATE)\n","\n","    avg_train_acc, avg_train_f1, avg_val_acc, avg_val_f1, ov_test_acc, ov_test_f1, best_accuracy, best_f1 = train_eval(train_data_loaders,\n","                                                                                                                        val_data_loaders,\n","                                                                                                                        test_data_loaders,\n","                                                                                                                        len(y_train),\n","                                                                                                                        len(y_val),\n","                                                                                                                        len(y_test),\n","                                                                                                                        epochs=NUM_TRAIN_EPOCHS)\n","\n","\n","\n","    folds_val_log.append(best_accuracy)\n","    folds_val_log_f1.append(best_f1)\n","    folds_encrypted_test_log.append(ov_test_acc)\n","    folds_encrypted_test_log_f1.append(ov_test_f1)\n","    print(\"Validation results:\", avg_val_acc)\n","\n","    del y_train, y_val, y_test\n","    del train_data_loaders, val_data_loaders, test_data_loaders\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HcdFGK3Cfam_"},"outputs":[],"source":["print(folds_encrypted_test_log)\n","print(folds_val_log)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DwFH7pSNk3iN"},"outputs":[],"source":["average = np.round(np.mean(folds_encrypted_test_log, axis=0), 6)\n","print(f'\\033[1mAverage of test acc. results:\\033[0m \\033[92m{average}\\033[0m')\n","\n","f1 = np.round(np.mean(folds_encrypted_test_log_f1, axis=0), 6)\n","print(f'\\033[1mAverage of test f1 results:\\033[0m \\033[92m{f1}\\033[0m')\n","\n","average = np.round(np.mean(folds_val_log, axis=0), 6)\n","print(f'\\033[1mAverage of val. acc. results:\\033[0m \\033[92m{average}\\033[0m')\n","\n","f1 = np.round(np.mean(folds_val_log_f1, axis=0), 6)\n","print(f'\\033[1mAverage of val. f1 results:\\033[0m \\033[92m{f1}\\033[0m')"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}