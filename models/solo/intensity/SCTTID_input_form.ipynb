{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"fmmVTB1VL2rP"},"outputs":[],"source":["!pip install transformers[sentencepiece]\n","!pip install torchinfo\n","!pip install cryptography"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MVEV-fv-ht-U"},"outputs":[],"source":["ed = 'MPQA2.0_cleaned'\n","k_fold = 5\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H9jeB-jRMgqE"},"outputs":[],"source":["import random\n","import time\n","from nltk import word_tokenize\n","import nltk\n","from nltk.corpus import stopwords\n","import re\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kMri775FK4TM"},"outputs":[],"source":["import gc\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import pandas as pd\n","from transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer, DataCollatorWithPadding, EarlyStoppingCallback, AutoConfig\n","from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","import json\n","from urllib.request import urlopen\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import statistics\n","import os\n","from tqdm import tqdm\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","from collections import defaultdict\n","\n","\n","%matplotlib inline\n","%config InlineBackend.figure_format='retina'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ddaiC5bbK9Tj"},"outputs":[],"source":["# Setup device\n","\n","device_string = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device_hf = 0 if torch.cuda.is_available() else -1\n","device = torch.device(device_string)\n","print(\"Device:\", device)\n","NUM_WORKERS = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DM3pdZ2lK_Qm"},"outputs":[],"source":["# Doclists\n","\n","ULA_SUBSET_DOCS = [\n","  'ula/119CWL041', 'ula/RindnerBonnie', 'ula/HistoryGreek',\n","  'ula/Article247_3500', 'ula/NapierDianne', 'ula/sw2071-UTF16-ms98-a-trans',\n","  'ula/118CWL050', 'ula/114CUL059', 'ula/110CYL067', 'ula/PolkMaria',\n","  'ula/116CUL034', 'ula/115CVL037', 'ula/118CWL049', 'ula/Article247_66',\n","  'ula/110CYL068', 'ula/113CWL017', 'ula/112C-L015', 'ula/115CVL036',\n","  'ula/115CVL035', 'ula/Article247_328', 'ula/114CUL060', 'ula/112C-L012',\n","  'ula/118CWL048', 'ula/ReidSandra', 'ula/112C-L016', 'ula/HistoryJerusalem',\n","  'ula/110CYL070', 'ula/sw2014-UTF16-ms98-a-trans', 'ula/112C-L014',\n","  'ula/117CWL008', 'ula/sw2078-UTF16-ms98-a-trans', 'ula/110CYL071',\n","  'ula/114CUL057', 'ula/116CUL032', 'ula/110CYL069', 'ula/117CWL009',\n","  'ula/110CYL072', 'ula/chapter-10', 'ula/116CUL033', 'ula/ch5',\n","  'ula/sw2015-ms98-a-trans', 'ula/113CWL018', 'ula/110CYL200',\n","  'ula/Article247_327', 'ula/114CUL058', 'ula/112C-L013', 'ula/Article247_500',\n","  'ula/Article247_400'\n","]\n","\n","ULA_LU_SUBSET_DOCS = [\n","  'ula/A1.E2-NEW', 'ula/wsj_1640.mrg-NEW', 'ula/AFGP-2002-600045-Trans',\n","  'ula/20000410_nyt-NEW', 'ula/20000415_apw_eng-NEW',\n","  'ula/AFGP-2002-602187-Trans', 'ula/20000815_AFP_ARB.0084.IBM-HA-NEW',\n","  'ula/CNN_AARONBROWN_ENG_20051101_215800.partial-NEW', 'ula/20000424_nyt-NEW',\n","  'ula/20000419_apw_eng-NEW', 'ula/20000416_xin_eng-NEW',\n","  'ula/enron-thread-159550', 'ula/wsj_2465', 'ula/AFGP-2002-600002-Trans',\n","  'ula/ENRON-pearson-email-25jul02', 'ula/im_401b_e73i32c22_031705-2',\n","  'ula/A1.E1-NEW', 'ula/CNN_ENG_20030614_173123.4-NEW-1',\n","  'ula/20000420_xin_eng-NEW', 'ula/IZ-060316-01-Trans-1',\n","  'ula/sw2025-ms98-a-trans.ascii-1-NEW', 'ula/SNO-525',\n","  'ula/AFGP-2002-600175-Trans', 'ula/602CZL285-1'\n","]\n","\n","XBANK_DOCS = [\n","  'xbank/wsj_0904', 'xbank/wsj_0760', 'xbank/wsj_0713', 'xbank/wsj_0709',\n","  'xbank/wsj_0706', 'xbank/wsj_0662', 'xbank/wsj_0558', 'xbank/wsj_0555',\n","  'xbank/wsj_0551', 'xbank/wsj_0542', 'xbank/wsj_0541', 'xbank/wsj_0332',\n","  'xbank/wsj_0292', 'xbank/wsj_0189', 'xbank/wsj_0316', 'xbank/wsj_0175',\n","  'xbank/wsj_0321', 'xbank/wsj_0176', 'xbank/wsj_0173', 'xbank/wsj_0026',\n","  'xbank/wsj_0324', 'xbank/wsj_0187', 'xbank/wsj_0356', 'xbank/wsj_0325',\n","  'xbank/wsj_0340', 'xbank/wsj_0679', 'xbank/wsj_0695', 'xbank/wsj_0661',\n","  'xbank/wsj_0570', 'xbank/wsj_0557', 'xbank/wsj_0751', 'xbank/wsj_0805',\n","  'xbank/wsj_0762', 'xbank/wsj_0736', 'xbank/wsj_0806', 'xbank/wsj_1040',\n","  'xbank/wsj_1039', 'xbank/wsj_1042', 'xbank/wsj_0568', 'xbank/wsj_0778',\n","  'xbank/wsj_0160', 'xbank/wsj_0136', 'xbank/wsj_0135', 'xbank/wsj_0127',\n","  'xbank/wsj_0122', 'xbank/wsj_0032', 'xbank/wsj_0150', 'xbank/wsj_0165',\n","  'xbank/wsj_0157', 'xbank/wsj_0151', 'xbank/wsj_0685', 'xbank/wsj_0168',\n","  'xbank/wsj_0167', 'xbank/wsj_0161', 'xbank/wsj_0152', 'xbank/wsj_0073',\n","  'xbank/wsj_0068', 'xbank/wsj_0171', 'xbank/wsj_0144', 'xbank/wsj_0991',\n","  'xbank/wsj_0923', 'xbank/wsj_0907', 'xbank/wsj_0811', 'xbank/wsj_0667',\n","  'xbank/wsj_0534', 'xbank/wsj_0924', 'xbank/wsj_0815', 'xbank/wsj_1038',\n","  'xbank/wsj_1035', 'xbank/wsj_1033', 'xbank/wsj_0527', 'xbank/wsj_0928',\n","  'xbank/wsj_0973', 'xbank/wsj_0950', 'xbank/wsj_0927', 'xbank/wsj_0376',\n","  'xbank/wsj_0660', 'xbank/wsj_0650', 'xbank/wsj_0266', 'xbank/wsj_0006',\n","  'xbank/wsj_0768', 'xbank/wsj_1073', 'xbank/wsj_0816', 'xbank/wsj_0610',\n","  'xbank/wsj_0583'\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PTrIkitUxLZh"},"outputs":[],"source":["def clean_text(txt, head):\n","    if txt == None:\n","        return None, None\n","    try:\n","        txt = re.sub(\"\\n\", \" \", txt)\n","        head = re.sub(\"\\n\", \" \", head)\n","        txt =  re.sub(\"\\t\", \" \", txt)\n","        head =  re.sub(\"\\t\", \" \", head)\n","        #\n","\n","        txt = re.sub(\"LU_ANNOTATE>\", \" \", txt)\n","        txt = re.sub(\"<LU_ANNOTATE>\", \" \", txt)\n","\n","        while txt.find('  ') > -1:\n","            txt =  re.sub(\"  \", \" \", txt)\n","        while head.find('  ') > -1:\n","            head =  re.sub(\"  \", \" \", head)\n","\n","        return txt, head\n","    except:\n","        print(txt, ' $$$ ', head)\n","        return txt, head\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GXu9fdfC7kYm"},"outputs":[],"source":["def change_misses(a, b):\n","    misses = {' ,': ', ',\n","              ' .': '. ',\n","              ' \\'s': '\\'s',\n","              '  ': ' '\n","    }\n","\n","    for miss in misses.keys():\n","        while a.find(miss) > -1:\n","            a = a.replace(miss, misses[miss])\n","\n","        while b.find(miss) > -1:\n","            b = b.replace(miss, misses[miss])\n","\n","    return a, b"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FE10Ljv9LCw5"},"outputs":[],"source":["# Config\n","\n","IGNORED_DOCS = ULA_SUBSET_DOCS + ULA_LU_SUBSET_DOCS + XBANK_DOCS # e.g. set it to ['non_fbis/06.11.16-17420'] to remove all objects with that specific document id\n","\n","ANNOT_DICT = {'expressive_subjectivity': 'expressive subjectivity',\n","              'speculation': 'speculation', 'other_attitude': 'other',\n","              'intention': 'intention', 'arguing': 'arguing',\n","              'agreement': 'agreement', 'sentiment': 'sentiment'}\n","\n","ANNOTS= {'expressive_subjectivity': [1, 0, 0, 0, 0],\n","              'intention': [0, 1, 0, 0, 0], 'arguing': [0, 0, 1, 0, 0],\n","              'agreement': [0, 0, 0, 1, 0], 'sentiment': [0, 0, 0, 0, 1]}\n","\n","CLASSES = {'medium': [0, 1, 0], 'medium-high': [0, 1, 1], 'low': [1, 0, 0],\n","           'high': [0, 0, 1], 'low-medium': [1, 1, 0],\n","           'high-extreme': [0, 0, 1], 'extreme': [0, 0, 1]}\n","\n","MODEL_NAME = 'bert-base-uncased'\n","PRE_TRAINED_MODEL_NAME = MODEL_NAME\n","FREEZE_LAYER_COUNT = None\n","BATCH_SIZE = 16\n","TRAIN_BATCH_SIZE = 16\n","VAL_BATCH_SIZE = 64\n","TEST_BATCH_SIZE = 1\n","LOGGING_STEPS = 400\n","EVAL_STRATEGY = 'steps'\n","SAVE_STRATEGY = 'steps'\n","LOAD_BEST_MODEL_AT_END = True\n","METRIC_FOR_BEST_MODEL = 'eval_micro/f1'\n","DROPOUT = 0.1\n","BCE_WEIGHT_EXPONENT = 0\n","NUM_TRAIN_EPOCHS = 4\n","CALLBACKS = [EarlyStoppingCallback(4)]\n","SEED = 0\n","TEST_SPLIT_SEED = 0\n","VAL_SPLIT_SEED = 0\n","DATA = ed\n","VAL_SIZE = 0.2\n","KEY = b'-TheMostSuperPowerfulKeyAvailableInTheWorld='"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rftfNM0kLEXY"},"outputs":[],"source":["data_name_to_google_drive_url = {\n","    'MPQA2.0_cleaned': '[Replace proper link here.]',\n","    'IDS': '[Replace proper link here.]'\n","}\n","\n","\n","# Get direct download link\n","def get_download_url_from_google_drive_url(google_drive_url):\n","    return f'https://drive.google.com/uc?id={google_drive_url.split(\"/\")[5]}&export=download&confirm=t'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"upmxL-y5rAFm"},"outputs":[],"source":["DATA = ed\n","IDS = 'IDS'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"04YGCCOOiH5-"},"outputs":[],"source":["# Fetch the dataset\n","import os\n","from os import chdir\n","from google.colab import drive\n","\n","FETCH_FROM_WEB = True ### Set it to true, to download the datasets from github and google drive ###\n","\n","if FETCH_FROM_WEB:\n","    data_url = get_download_url_from_google_drive_url(data_name_to_google_drive_url[DATA])\n","    response = urlopen(data_url)\n","    data = json.loads(response.read())\n","    #\n","\n","    ids_url = get_download_url_from_google_drive_url(data_name_to_google_drive_url[IDS])\n","    response = urlopen(ids_url)\n","    ids = json.loads(response.read())\n","else:\n","    file_address = '..\\\\json2csds\\\\data.json'\n","    with open(file_address) as file:\n","        csds_collection = json.load(file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FQZBWGN5ro0s"},"outputs":[],"source":["k = 1\n","print('Number of trainset: {}'.format(len(ids['IDs_trainset_fold_{}'.format(k)])))\n","print('Number of validationset: {}'.format(len(ids['IDs_validationset_fold_{}'.format(k)])))\n","print('Number of testset: {}'.format(len(ids['IDs_testset_fold_{}'.format(k)])))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2u166MmiLI6n"},"outputs":[],"source":["def set_seed():\n","    np.random.seed(SEED)\n","    torch.manual_seed(SEED)\n","    torch.cuda.manual_seed(SEED)\n","    torch.backends.cudnn.deterministic = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0zl95p4rZylV"},"outputs":[],"source":["def place_char(a, b, beg, end, c=None):\n","    if a is None:\n","        print(a, ' $$ ', b)\n","        return None, None\n","    else:\n","\n","        a = ANNOT_DICT[c] + ' ' + eos_token + ' ' + a[0:beg] + ' ' + eos_token + ' ' +  b + ' ' + eos_token + ' ' + a[end+1:]\n","\n","        return a, b"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"msEpoHpzYqAK"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n","###########\n","init_token = ' ' + tokenizer.cls_token\n","eos_token = ' ' + tokenizer.sep_token"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xo5l2ENNeZwW"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"stqfzu1QNyYS"},"outputs":[],"source":["for item in data['csds_objects']:\n","    print(item.keys())\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UklQSIWuSHJr"},"outputs":[],"source":["import re\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.tokenize import WhitespaceTokenizer\n","from nltk.tokenize.treebank import TreebankWordDetokenizer\n","detokenizer = TreebankWordDetokenizer()\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t6JvEURgwW-4"},"outputs":[],"source":["!pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bJN-RNDUsZWU"},"outputs":[],"source":["# Read Trainset & Validationset & testset of each fold\n","\n","for k in range(1, k_fold+1):\n","    X_text, X_head, X = [], [], []\n","\n","    X_train, X_text_train, X_head_train, X_annot_train, X_unique_id_train, y_train = [], [], [], [], [], []\n","    X_val, X_text_val, X_head_val, X_annot_val, X_unique_id_val, y_val = [], [], [], [], [], []\n","    X_test, X_text_test, X_head_test, X_annot_test, X_unique_id_test, y_test = [], [], [], [], [], []\n","\n","    for item in data['csds_objects']:\n","        if item['unique_id'] in ids['IDs_trainset_fold_{}'.format(k)]:\n","\n","            #\n","            start, end = item['w_head_span'][0], item['w_head_span'][1]\n","            start_index = len(detokenizer.detokenize(item['w_text'][0: start]))\n","            end_index = len(detokenizer.detokenize(item['w_text'][0:end]))\n","            text = detokenizer.detokenize(item['w_text'])\n","\n","\n","            doc_id = item['doc_id']\n","            text = text\n","            head = text[start_index: end_index]\n","            annotype = item['annotation_type']\n","            intensity = item['intensity']\n","            unique_id = item['unique_id']\n","            head_start = start_index\n","            head_end = end_index\n","            #\n","            a, b = place_char(text, head, head_start, head_end, c=annotype)\n","\n","            if a:\n","\n","                X_train.append(a)\n","            else:\n","                print('Error')\n","                X_train.append(text)\n","\n","            X_head_train.append(head)\n","            X_text_train.append(text)\n","            X_annot_train.append(ANNOTS[annotype])\n","            y_train.append(CLASSES[intensity])\n","            X_unique_id_train.append(unique_id)\n","\n","        elif item['unique_id'] in ids['IDs_validationset_fold_{}'.format(k)]:\n","            #\n","            start, end = item['w_head_span'][0], item['w_head_span'][1]\n","            start_index = len(detokenizer.detokenize(item['w_text'][0: start]))\n","            end_index = len(detokenizer.detokenize(item['w_text'][0:end]))\n","            text = detokenizer.detokenize(item['w_text'])\n","\n","\n","            doc_id = item['doc_id']\n","            text = text\n","            head = text[start_index: end_index]\n","            annotype = item['annotation_type']\n","            intensity = item['intensity']\n","            unique_id = item['unique_id']\n","            head_start = start_index\n","            head_end = end_index\n","            #\n","            a, b = place_char(text, head, head_start, head_end, c=annotype)\n","\n","            if a:\n","                X_val.append(a)\n","            else:\n","                print('Error')\n","                X_val.append(text)\n","\n","            X_head_val.append(head)\n","            X_text_val.append(text)\n","            X_annot_val.append(ANNOTS[annotype])\n","            y_val.append(CLASSES[intensity])\n","            X_unique_id_val.append(unique_id)\n","\n","\n","        elif item['unique_id'] in ids['IDs_testset_fold_{}'.format(k)]:\n","            #\n","            start, end = item['w_head_span'][0], item['w_head_span'][1]\n","            start_index = len(detokenizer.detokenize(item['w_text'][0: start]))\n","            end_index = len(detokenizer.detokenize(item['w_text'][0:end]))\n","            text = detokenizer.detokenize(item['w_text'])\n","\n","\n","            doc_id = item['doc_id']\n","            text = text\n","            head = text[start_index: end_index]\n","            annotype = item['annotation_type']\n","            intensity = item['intensity']\n","            unique_id = item['unique_id']\n","            head_start = start_index\n","            head_end = end_index\n","            #\n","            a, b = place_char(text, head, head_start, head_end, c=annotype)\n","\n","            if a:\n","                X_test.append(a)\n","            else:\n","                print('Error')\n","                X_test.append(text)\n","\n","            X_head_test.append(head)\n","            X_text_test.append(text)\n","            X_annot_test.append(ANNOTS[annotype])\n","            y_test.append(CLASSES[intensity])\n","            X_unique_id_test.append(unique_id)\n","\n","\n","    # save in json\n","\n","    path = \"/content/drive/My Drive/Overall_Final_Files/kfold/\"\n","\n","    result = {\"head\": X_head_train, \"text\": X_text_train, \"all\": X_train,\n","              \"annotationType\": X_annot_train, \"target\": y_train,\n","              \"uniqueID\": X_unique_id_train}\n","\n","\n","    with open(path + DATA + '_trainset_fold_{}.json'.format(k), 'w', encoding='utf-8') as json_file:\n","        json.dump(result, json_file, ensure_ascii=False, indent=4)\n","\n","\n","    result = {\"head\": X_head_val, \"text\": X_text_val, \"all\": X_val,\n","              \"annotationType\": X_annot_val, \"target\": y_val,\n","              \"uniqueID\": X_unique_id_val}\n","\n","    with open(path + DATA + '_validationset_fold_{}.json'.format(k), 'w', encoding='utf-8') as json_file:\n","        json.dump(result, json_file, ensure_ascii=False, indent=4)\n","\n","\n","    result = {\"head\": X_head_test, \"text\": X_text_test, \"all\": X_test,\n","              \"annotationType\": X_annot_test, \"target\": y_test,\n","              \"uniqueID\": X_unique_id_test}\n","\n","    with open(path + DATA + '_testset_fold_{}.json'.format(k), 'w', encoding='utf-8') as json_file:\n","        json.dump(result, json_file, ensure_ascii=False, indent=4)\n","\n","    k += 1"]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}